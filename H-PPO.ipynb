{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyO/Zrr4ZIKzCHgBd1+A4YrH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Hybrid Proximal Policy Optimization"],"metadata":{"id":"GGD4D1CwVT3b"}},{"cell_type":"markdown","source":["## Guide d'utilisation\n","\n","**/!\\ ATTENTION /!\\**\n","\n","Ce Google colab ne fonctionne pas dès la première exécution.\n","\n","Veuillez suivre les étapes suivantes pour le faire fonctionner correctement:\n","1. Cliquer sur `Runtime` puis `Run all`.\n","2. Une erreur va être générée.\n","3. Cliquer sur `Runtime` puis `Restart session`\n","4. A présent vous pouvez utiliser le jupyter notebook."],"metadata":{"id":"VTBXsaif07Kd"}},{"cell_type":"markdown","source":["## Imports et installations\n"],"metadata":{"id":"2mGukyoY03mP"}},{"cell_type":"code","source":["import gym"],"metadata":{"id":"XJJydXIJ3ps7","executionInfo":{"status":"ok","timestamp":1706293247159,"user_tz":-60,"elapsed":18,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9Zzsp0IkVPJc","executionInfo":{"status":"ok","timestamp":1706293247479,"user_tz":-60,"elapsed":336,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}}},"outputs":[],"source":["if gym.__version__ != \"0.10.5\":\n","  !pip install gym==0.10.5\n","  raise Exception(\"Le Google Colab doit être rééxécuté, comme indiqué à l'étape 3 du guide d'utilisation.\")"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Drive connection\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Path when the files are in a shared drive\n","path = '\"/content/drive/Shareddrives/ING3 IA & applications/\"'\n","\n","# Move in directories\n","%cd $path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AM4AjF-72KF8","executionInfo":{"status":"ok","timestamp":1706293250482,"user_tz":-60,"elapsed":3007,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}},"outputId":"bd56bbf1-5741-44fe-d99e-f3021003ae61"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/Shareddrives/ING3 IA & applications\n"]}]},{"cell_type":"code","source":["!pip install -e code/gym-goal"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLVBk3zY2O0S","executionInfo":{"status":"ok","timestamp":1706293269629,"user_tz":-60,"elapsed":19159,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}},"outputId":"5d9698c8-94ce-44c7-a798-97672cf7219c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/drive/Shareddrives/ING3%20IA%20%26%20applications/code/gym-goal\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-goal==0.0.1) (0.10.5)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from gym-goal==0.0.1) (2.5.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gym-goal==0.0.1) (1.23.5)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-goal==0.0.1) (2.31.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gym->gym-goal==0.0.1) (1.16.0)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-goal==0.0.1) (2.0.10)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym->gym-goal==0.0.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym->gym-goal==0.0.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym->gym-goal==0.0.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym->gym-goal==0.0.1) (2023.11.17)\n","Installing collected packages: gym-goal\n","  Attempting uninstall: gym-goal\n","    Found existing installation: gym-goal 0.0.1\n","    Uninstalling gym-goal-0.0.1:\n","      Successfully uninstalled gym-goal-0.0.1\n","  Running setup.py develop for gym-goal\n","Successfully installed gym-goal-0.0.1\n"]}]},{"cell_type":"code","source":["import gym_goal\n","from matplotlib import pyplot as plt\n","import numpy as np\n","\n","from datetime import datetime as dt\n","\n","import torch\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Exécution sur {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gpx4nYsL2Swg","executionInfo":{"status":"ok","timestamp":1706293273167,"user_tz":-60,"elapsed":3550,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}},"outputId":"f89f44f1-969e-418b-ec13-6b02c7d25a9b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Exécution sur cpu\n"]}]},{"cell_type":"markdown","source":["## Environnement\n","\n","Environnement dans lequel l'agent évolue : [Robot Soccer Goal](https://github.com/cycraig/gym-goal)\n"],"metadata":{"id":"7LGr06Mu2bF1"}},{"cell_type":"code","source":["env = gym.make('Goal-v0')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wazTYtT22cn2","executionInfo":{"status":"ok","timestamp":1706293273169,"user_tz":-60,"elapsed":23,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}},"outputId":"56e82bb2-ad82-4a2a-a827-5453f387162a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["pygame 2.5.2 (SDL 2.28.2, Python 3.10.12)\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n","  result = entry_point.load(False)\n"]}]},{"cell_type":"markdown","source":["## Modèle"],"metadata":{"id":"gdQ2C3PPdTa8"}},{"cell_type":"code","source":["# Simple Neural Network used by default in PPO implementation\n","class LinearNN(torch.nn.Module):\n","  \"\"\"\n","  Simple linear neural network for regression\n","  \"\"\"\n","  def __init__(self, input_dim, output_dim):\n","    \"\"\"\n","    Create a LinearNN object.\n","\n","    #### Parameters:\n","    :input_dim: positive integer, dimension of the input data\n","    :output_dim: positive integer, dimension of the output data\n","\n","    #### Return:\n","    None\n","    \"\"\"\n","    super(LinearNN, self).__init__()\n","    self.model = torch.nn.Sequential(\n","          torch.nn.Linear(input_dim,64),\n","          torch.nn.ReLU(),\n","          torch.nn.Linear(64,64),\n","          torch.nn.ReLU(),\n","          torch.nn.Linear(64,output_dim)\n","        )\n","\n","  def forward(self, x):\n","    \"\"\"\n","    Runs a forward pass throw the model.\n","\n","    #### Parameters:\n","    :x: input data\n","\n","    #### Return:\n","    Data computed by the model\n","    \"\"\"\n","    if isinstance(x, np.ndarray):\n","      x = torch.tensor(x, dtype=torch.float)\n","    return self.model(x)\n","\n","\n","class LinearClassifier(torch.nn.Module):\n","  \"\"\"\n","  Simple linear neural network for classification\n","  \"\"\"\n","  def __init__(self, input_dim, output_dim):\n","    \"\"\"\n","    Create a LinearNN object.\n","\n","    #### Parameters:\n","    :input_dim: positive integer, dimension of the input data\n","    :output_dim: positive integer, dimension of the output data\n","\n","    #### Return:\n","    None\n","    \"\"\"\n","    super(LinearClassifier, self).__init__()\n","    self.model = torch.nn.Sequential(\n","          torch.nn.Linear(input_dim,64),\n","          torch.nn.ReLU(),\n","          torch.nn.Linear(64,64),\n","          torch.nn.ReLU(),\n","          torch.nn.Linear(64,output_dim),\n","          torch.nn.Softmax(dim=0)\n","        )\n","\n","  def forward(self, x):\n","    \"\"\"\n","    Runs a forward pass throw the model.\n","\n","    #### Parameters:\n","    :x: input data\n","\n","    #### Return:\n","    Data computed by the model\n","    \"\"\"\n","    if isinstance(x, np.ndarray):\n","      x = torch.tensor(x, dtype=torch.float)\n","    return self.model(x)\n","\n","\n","\n","def flatten(args):\n","    try:\n","        iter(args)\n","        final = []\n","        for arg in args:\n","            final += flatten(arg)\n","        return tuple(final)\n","    except TypeError:\n","        return (args, )\n","\n","\n","\n","\n","# H-PPO implementation\n","class HPPO:\n","  \"\"\"\n","  Class to create Hybrid Proximal Policy Optimization models for Deep Reinforcement Learning\n","\n","  It is only adapted to run on Robot Soccer Goal environnement : https://github.com/cycraig/gym-goal\n","  \"\"\"\n","\n","  # TODO: rewrite documentation\n","\n","  # Private methods, name starting with '__'\n","\n","  def __init__(self,\n","              env,\n","              actor_classifier=None,\n","              actor_regression=None,\n","              critic=None,\n","              path_to_actor_classifier=\"\",\n","              path_to_actor_regression=\"\",\n","              path_to_critic=\"\",\n","              actor_classifier_optim=torch.optim.Adam,\n","              actor_regression_optim=torch.optim.Adam,\n","              critic_optim=torch.optim.Adam,\n","              actor_classifier_hyperparams={\"lr\":0.005},\n","              actor_regression_hyperparams={\"lr\":0.005},\n","              critic_hyperparams={\"lr\":0.005},\n","              gamma=0.95\n","  ):\n","    \"\"\"\n","    H-PPO generator for a given environment.\n","\n","    #### Parameters:\n","    :env: gym environment where the model will evolve\n","    :actor_classifier: Neural Network class implementing torch.nn.Module . By default create a simple neural network if there is no path for saved model.\n","    :actor_regression: Neural Network class implementing torch.nn.Module . By default create a simple neural network if there is no path for saved model.\n","    :critic: Neural Network class implementing torch.nn.Module . By default create a simple neural network if there is no path for saved model.\n","    :path_to_actor_classifier: string, path to a saved actor classifier model. By default use the given `actor_classifier` model.\n","    :path_to_actor_regression: string, path to a saved actor regression model. By default use the given `actor_regression` model.\n","    :path_to_critic: string, path to a saved critic model. By default use the given `critic` model.\n","    :actor_optim_classifier: torch.optim.Optimizer object, optimizer for the actor classifier model\n","    :actor_optim_regression: torch.optim.Optimizer object, optimizer for the actor regression model\n","    :critic_optim: torch.optim.Optimizer object, optimizer for the critic model\n","    :actor_classifier_hyperparams: dictionnary of the actor's hyperparameters\n","    :actor_regression_hyperparams: dictionnary of the actor's hyperparameters\n","    :critic_hyperparams: dictionnary of the critic's hyperparameters\n","\n","    #### Return:\n","    None\n","    \"\"\"\n","    # Extract the environment\n","    self.env = env\n","    observation_space = len(flatten(env.reset()))\n","    regression_space = len(flatten(env.action_space.sample()[1]))\n","\n","    # Build the actor and critic models\n","    if actor_classifier:\n","      self.actor_classifier = actor_classifier(\n","          observation_space,\n","          3\n","      )\n","    else:\n","      try:\n","        self.actor_classifier = torch.load(path_to_actor_classifier)\n","      except:\n","        self.actor_classifier = LinearClassifier(\n","            observation_space,\n","            3\n","        )\n","    if actor_regression:\n","      self.actor_regression = actor_regression(\n","          observation_space,\n","          regression_space\n","      )\n","    else:\n","      try:\n","        self.actor_regression = torch.load(path_to_actor_regression)\n","      except:\n","        self.actor_regression = LinearNN(\n","            observation_space,\n","            regression_space\n","        )\n","    if critic:\n","      self.critic = critic(\n","          observation_space,\n","          1\n","      )\n","    else:\n","      try:\n","        self.critic = torch.load(path_to_critic)\n","      except:\n","        self.critic = LinearNN(\n","            observation_space,\n","            1\n","        )\n","\n","    # Create optimizers for the neural networks\n","    self.actor_regression_optim = actor_regression_optim(self.actor_regression.parameters(), **actor_regression_hyperparams)\n","    self.actor_classifier_optim = actor_classifier_optim(self.actor_classifier.parameters(), **actor_classifier_hyperparams)\n","    self.critic_optim = critic_optim(self.critic.parameters(), **critic_hyperparams)\n","\n","    # Initialize the covariance matrix used to query the actor for actions\n","    self.cov_mat_classifier = torch.diag(\n","      torch.full(\n","          size=(3,),\n","          fill_value=0.5\n","      )\n","    )\n","    self.cov_mat_regression = torch.diag(\n","      torch.full(\n","          size=(regression_space,),\n","          fill_value=0.5\n","      )\n","    )\n","\n","    self.gamma = gamma\n","\n","\n","  def __random_action(self, obs):\n","    \"\"\"\n","    Compute a random action based on a prediction made by the actor model.\n","\n","    #### Parameters:\n","    :obs: observation of the current environment state\n","\n","    #### Return:\n","    A tupple of:\n","    - the continuous action to take, as a numpy array\n","    - the log probability of the selected continuous action in the distribution\n","    - the discontinuous action to take, as a numpy array\n","    - the log probability of the selected discontinuous action in the distribution\n","    \"\"\"\n","    obs=torch.tensor(flatten(obs), dtype=torch.float32)\n","\n","    # Regression\n","    # Create a distribution of actions based on the actor prediction and the PPO covariance matrix\n","    dist = torch.distributions.MultivariateNormal(self.actor_regression(obs), self.cov_mat_regression)\n","    # Pick a random action from the distribution\n","    action_regression = dist.sample()\n","    # Calculate the log probability for that action\n","    log_prob_regression = dist.log_prob(action_regression)\n","\n","    # Classification\n","    # Create a distribution of actions based on the actor prediction and the PPO covariance matrix\n","    dist = torch.distributions.MultivariateNormal(\n","        self.actor_classifier(obs),\n","        self.cov_mat_classifier)\n","    # Pick a random action from the distribution\n","    action_classifier = dist.sample()\n","    # Calculate the log probability for that action\n","    log_prob_classifier = dist.log_prob(action_classifier)\n","    return action_regression.detach().numpy(), log_prob_regression.detach(), action_classifier.detach().numpy(), log_prob_classifier.detach()\n","\n","\n","  def batch_exploration(self, batch_size=5, max_actions=1000, verbose=0):\n","    \"\"\"\n","    Compute several simulation then group them in a batch.\n","\n","    #### Parameters:\n","    :batch_size: positive integer, number of simulation to run in a batch\n","    :max_actions: positive integer, maximum of actions per simulation\n","    :verbose: integer, display option: if superior than 0 then print the simulations' number\n","\n","    #### Return:\n","    A tupple of tensors:\n","    - batch_obs, the environment's states\n","    - batch_acts_classifier, the discontinuous actions made by the H-PPO\n","    - batch_log_probs_classifier, the probability of each discontinuous action\n","    - batch_acts_regression, the continuous actions made by the H-PPO\n","    - batch_log_probs_regression, the probability of each continuous action\n","    - batch_rtgs, the rewards-to-go\n","    \"\"\"\n","    batch_obs = []\n","    batch_acts_regression = []\n","    batch_acts_classifier = []\n","    batch_log_probs_regression = []\n","    batch_log_probs_classifier = []\n","    batch_rews = []\n","    batch_rtgs = []\n","\n","    for batch in range(batch_size):\n","      if verbose > 0:\n","        print(\"Simulation \", batch+1,\"/\",batch_size)\n","\n","      rews_simu = []\n","      obs = self.env.reset()\n","      end_simu = False\n","      nb_actions = 0\n","\n","      # Run a simulation\n","      while (nb_actions < max_actions) and not(end_simu):\n","        batch_obs.append(flatten(obs))\n","        action_regression, log_prob_regression, action_classifier, log_prob_classifier = self.__random_action(obs)\n","        action = (np.argmax(action_classifier), (action_regression[0:2], action_regression[2], action_regression[3]))\n","        obs, rew, end_simu, _ = self.env.step(action)\n","        rews_simu.append(rew)\n","        batch_acts_regression.append(action_regression)\n","        batch_log_probs_regression.append(log_prob_regression)\n","        batch_acts_classifier.append(action_classifier)\n","        batch_log_probs_classifier.append(log_prob_classifier)\n","        nb_actions += 1\n","\n","      # # Compute rewards to go\n","      # discounted_reward = 0\n","      # index_rtgs = len(batch_rews)\n","      # for rew in reversed(rews_simu):\n","      #   discounted_reward = rew + discounted_reward * self.gamma\n","      #   batch_rtgs.insert(index_rtgs, discounted_reward)\n","\n","      batch_rews.append(rews_simu)\n","\n","    # Compute rewards to go\n","    batch_rtgs = []\n","    for ep_rews in reversed(batch_rews):\n","      discounted_reward = 0\n","      for rew in reversed(ep_rews):\n","        discounted_reward = rew + discounted_reward * self.gamma\n","        batch_rtgs.insert(0, discounted_reward)\n","\n","    # Reshape data as tensors\n","    batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n","    batch_acts_classifier = torch.tensor(batch_acts_classifier, dtype=torch.float)\n","    batch_log_probs_classifier = torch.tensor(batch_log_probs_classifier, dtype=torch.float)\n","    batch_acts_regression = torch.tensor(batch_acts_regression, dtype=torch.float)\n","    batch_log_probs_regression = torch.tensor(batch_log_probs_regression, dtype=torch.float)\n","    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n","    return batch_obs, batch_acts_classifier, batch_log_probs_classifier, batch_acts_regression, batch_log_probs_regression, batch_rtgs, batch_rews\n","\n","\n","  # Public methods\n","\n","  def fit(self, explorations=3, batch_size=3, max_actions=1600, epochs=5, clip=0.2, saving_path=\"\", verbose=0):\n","    \"\"\"\n","    Train the model.\n","\n","    #### Parameters:\n","    :exploration: positive integer, number of times that the model will explore the environment\n","    :batch_size: positive integer, size of the batch used to train the actor and critic networks\n","    :max_actions: positive integer, maximum of actions per exploration\n","    :epochs: positive integer corresponding to the number of epochs to train the actor and critic networks\n","    :clip: float, clip value for advantage during loss computing\n","    :saving_path: string, path to the saved file of the actor and critic models and a report of the training\n","    :verbose: positive integer, display option\n","\n","    #### Return:\n","    Dictionnary reporting the training results\n","    \"\"\"\n","    report_data = {\n","        \"ppo\" : {\n","            \"environment\" : str(self.env.spec),\n","            \"gamma\" : float(self.gamma)\n","        },\n","        \"actor_regression_model\" : {\n","            \"file\" : \"\",\n","            \"optimizer\" : str(self.actor_regression_optim),\n","            \"hyperparameters\" : str(dict(self.actor_regression_optim.state_dict()))\n","        },\n","        \"actor_classifier_model\" : {\n","            \"file\" : \"\",\n","            \"optimizer\" : str(self.actor_classifier_optim),\n","            \"hyperparameters\" : str(dict(self.actor_classifier_optim.state_dict()))\n","        },\n","        \"critic_model\" : {\n","            \"file\" : \"\",\n","            \"optimizer\" : str(self.critic_optim),\n","            \"hyperparameters\" : str(dict(self.critic_optim.state_dict()))\n","        },\n","        \"fit_hyperparameters\": {\n","          \"explorations\" : explorations,\n","          \"batch_size\" : batch_size,\n","          \"max_actions\": max_actions,\n","          \"epochs\" : epochs,\n","          \"clip\" : float(clip)\n","        },\n","        \"results\" : {\n","          \"computing_time_seconds\" : int(dt.timestamp(dt.now())),\n","          \"timesteps\" : 0,\n","          \"batch_lens\" : [],\n","          \"avg_batch_rewards\" : [],\n","          \"avg_actor_classifier_losses\" : [],\n","          \"avg_actor_regression_losses\" : [],\n","          \"avg_critic_losses\" : []\n","        }\n","    }\n","\n","    for k in range(explorations):\n","      if verbose > 0:\n","        print(\"-----------------------------------------------------\")\n","        print(\"- Exploration \",k+1,\"/\",explorations,\":\")\n","\n","      batch_obs, batch_acts_classifier, batch_log_probs_classifier, batch_acts_regression, batch_log_probs_regression, batch_rtgs, batch_rews = self.batch_exploration(\n","          batch_size=batch_size,\n","          max_actions=max_actions,\n","          verbose=verbose-1\n","      )\n","\n","      report_data[\"results\"][\"timesteps\"] += len(batch_obs)\n","      report_data[\"results\"][\"batch_lens\"].append(int(len(batch_obs)))\n","      report_data[\"results\"][\"avg_batch_rewards\"].append(float(np.mean(flatten(batch_rews))))\n","\n","      V = self.critic(batch_obs).squeeze()\n","      A_k = batch_rtgs - V.detach()\n","\n","      # Advantages normalization\n","      A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n","\n","      actor_classifier_losses = []\n","      actor_regression_losses = []\n","      critic_losses = []\n","      for t in range(epochs):\n","        if verbose > 2:\n","          print(\"Epoch \",t+1,\"/\",epochs)\n","\n","        V = self.critic(batch_obs).squeeze()\n","        log_pi_t_classifier = torch.distributions.MultivariateNormal(self.actor_classifier(batch_obs), self.cov_mat_classifier).log_prob(batch_acts_classifier)\n","        log_pi_t_regression = torch.distributions.MultivariateNormal(self.actor_regression(batch_obs), self.cov_mat_regression).log_prob(batch_acts_regression)\n","\n","        # Calculate losses\n","        pi_ratio_classifier = torch.exp(log_pi_t_classifier - batch_log_probs_classifier)\n","        pi_ratio_regression = torch.exp(log_pi_t_regression - batch_log_probs_regression)\n","        surrogate_losses_frac_pi_classifier =  pi_ratio_classifier * A_k\n","        surrogate_losses_frac_pi_regression =  pi_ratio_regression * A_k\n","        surrogate_losses_g_classifier = torch.clamp(pi_ratio_classifier, 1 - clip, 1 + clip) * A_k\n","        surrogate_losses_g_regression = torch.clamp(pi_ratio_regression, 1 - clip, 1 + clip) * A_k\n","        actor_loss_classifier = (-torch.min(surrogate_losses_frac_pi_classifier, surrogate_losses_g_classifier)).mean()\n","        actor_loss_regression = (-torch.min(surrogate_losses_frac_pi_regression, surrogate_losses_g_regression)).mean()\n","        critic_loss = torch.nn.MSELoss()(V, batch_rtgs)\n","\n","        # Updating neural networks\n","        self.actor_classifier_optim.zero_grad()\n","        self.actor_regression_optim.zero_grad()\n","        actor_loss_classifier.backward(retain_graph=True)\n","        actor_loss_regression.backward(retain_graph=True)\n","        self.actor_classifier_optim.step()\n","        self.actor_regression_optim.step()\n","        self.critic_optim.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optim.step()\n","\n","        actor_classifier_losses.append(actor_loss_classifier.detach())\n","        actor_regression_losses.append(actor_loss_regression.detach())\n","        critic_losses.append(critic_loss.detach())\n","\n","      report_data[\"results\"][\"avg_actor_regression_losses\"].append(float(np.mean(actor_regression_losses)))\n","      report_data[\"results\"][\"avg_actor_classifier_losses\"].append(float(np.mean(actor_classifier_losses)))\n","      report_data[\"results\"][\"avg_critic_losses\"].append(float(np.mean(critic_losses)))\n","\n","    if verbose > 0:\n","      print(\"-----------------------------------------------------\\n\")\n","\n","    report_data[\"results\"][\"computing_time_seconds\"] = int(dt.timestamp(dt.now())) - report_data[\"results\"][\"computing_time_seconds\"]\n","\n","    if saving_path:\n","      saving_timestamp = int(dt.timestamp(dt.now()))\n","      self.save(\n","          saving_path,\n","          actor_regression_name=\"actor_regression_model_\"+str(saving_timestamp),\n","          actor_classifier_name=\"actor_classifier_model_\"+str(saving_timestamp),\n","          critic_name=\"critic_model_\"+str(saving_timestamp)\n","      )\n","      report_data[\"actor_regression_model\"][\"file\"] = \"actor_regression_model_\"+str(saving_timestamp)+\".pth\"\n","      report_data[\"actor_classifier_model\"][\"file\"] = \"actor_classifier_model_\"+str(saving_timestamp)+\".pth\"\n","      report_data[\"critic_model\"][\"file\"] = \"critic_model_\"+str(saving_timestamp)+\".pth\"\n","      self.report(report_data, saving_path, report_name=\"report_\"+str(saving_timestamp))\n","\n","    return report_data[\"results\"]\n","\n","\n","  def predict(self, obs):\n","    \"\"\"\n","    Predict an action based on the given observation.\n","\n","    #### Parameters:\n","    :obs: observation of the gym environment\n","\n","    #### Returns:\n","    gym action\n","    \"\"\"\n","    obs=torch.tensor(flatten(obs), dtype=torch.float32)\n","    action_regression = self.actor_regression(obs)\n","    action_classifier = self.actor_classifier(obs)\n","    action = (\n","        action_classifier.detach().numpy().argmax(),\n","        (\n","            action_regression[0:2].detach().numpy(),\n","            action_regression[2].detach().numpy(),\n","            action_regression[3].detach().numpy()\n","        )\n","    )\n","    return action\n","\n","\n","  def evaluate(self, max_actions=1000, win_limit=1, render=None, games_limit=100):\n","    \"\"\"\n","    Evaluate the PPO model's performance.\n","\n","    #### Parameters:\n","    :max_actions: positive integer, number of actions allowed in the environment\n","    :win_limit: positive integer, limit of times that the model is allowed to win the ultimate reward of the environment\n","    :render: render_mode for gym environment, choose a render_mode\n","    :games_limit: positive integer, maximum of playable games during the evaluation\n","\n","    #### Return:\n","    Tupple containing:\n","    - the sum of the rewards\n","    - the number of done actions during the evaluation\n","    - the number of times that the model won the ultimate reward of the environment\n","    - the render of the environment\n","    \"\"\"\n","    nb_actions = 0\n","    won_games = 0\n","    total_rewards = 0\n","    games = 0\n","    obs = self.env.reset()\n","    while (nb_actions < max_actions) and (won_games < win_limit) and (games < games_limit):\n","      action = self.predict(obs)\n","      obs, rew, end, _ = self.env.step(action)\n","      total_rewards += rew\n","      if rew == 50:\n","        won_games += 1\n","      games += int(end)\n","      nb_actions += 1\n","\n","    return total_rewards, nb_actions, won_games, self.env.render()\n","\n","\n","  def report(\n","      self,\n","      data,\n","      path,\n","      report_name=\"report_\"+str(int(dt.timestamp(dt.now()))),\n","      verbose=0\n","  ):\n","    \"\"\"\n","    Save a json report file.\n","\n","    #### Parameters:\n","    :data: dictionnary of the data to save\n","    :path: string, path to the folder where the file will be saved\n","    :report_name: string, name of the report\n","    :verbose: integer, display option\n","\n","    #### Return:\n","    None\n","    \"\"\"\n","    import json\n","\n","    report_path = path+\"/\"+report_name\n","    with open(report_path+\".json\", \"w\") as outfile:\n","      json.dump(data, outfile)\n","\n","\n","  def save(\n","      self,\n","      path,\n","      actor_regression_name=\"actor_regression_model_\"+str(int(dt.timestamp(dt.now()))),\n","      actor_classifier_name=\"actor_classifier_model_\"+str(int(dt.timestamp(dt.now()))),\n","      critic_name=\"critic_model_\"+str(int(dt.timestamp(dt.now())))\n","  ):\n","    \"\"\"\n","    Save the actor and critic models in files.\n","\n","    #### Parameters:\n","    :path: string, path where the files will be created\n","    :actor_regression_name: string, file's name where the regressor actor model will be saved\n","    :actor_classifier_name: string, file's name where the classifier actor model will be saved\n","    :critic_name: string, file's name where the critic model will be saved\n","\n","    #### Return:\n","    None\n","    \"\"\"\n","    torch.save(self.actor_regression, path+\"/\"+actor_regression_name+\".pth\")\n","    torch.save(self.actor_classifier, path+\"/\"+actor_classifier_name+\".pth\")\n","    torch.save(self.critic, path+\"/\"+critic_name+\".pth\")\n","\n","\n"],"metadata":{"id":"6V1esTBIfr6n","executionInfo":{"status":"ok","timestamp":1706293273490,"user_tz":-60,"elapsed":333,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Génération"],"metadata":{"id":"ZpXvpOEldZV_"}},{"cell_type":"markdown","source":["Génération d'un nouveau modèle ou import d'une sauvegarde"],"metadata":{"id":"JWUjB9jWd93V"}},{"cell_type":"code","source":["# @markdown Vous pouvez importer un modèle pré-entraîné du drive ou en générer un nouveau.\n","\n","# @markdown Les modèles pré-entraînés se situent dans le répertoire `code/models` du drive partagé. Ils portent des noms ayant le format : `NAME_model_TIMESTAMP.pth`.\n","# @markdown\n","\n","# @markdown - Remplir le champs ci-dessous avec le `TIMESTAMP` des modèles que vous souhaitez importer.\n","model_timestamp = 1706287566 # @param {type:\"integer\"}\n","# @markdown - Cocher cette case si vous préférez générer un nouveau modèle non entraîné.\n","new_model = False # @param {type:\"boolean\"}\n"],"metadata":{"id":"kneNXI9ieD_q","executionInfo":{"status":"ok","timestamp":1706293273492,"user_tz":-60,"elapsed":10,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["if new_model:\n","  # Create a new model\n","  model = HPPO(env)\n","else:\n","  # Import an existing one\n","  model = HPPO(\n","      env,\n","      path_to_actor_classifier=\"code/models/actor_classifier_model_\"+str(model_timestamp)+\".pth\",\n","      path_to_actor_regression=\"code/models/actor_regression_model_\"+str(model_timestamp)+\".pth\",\n","      path_to_critic=\"code/models/critic_model_\"+str(model_timestamp)+\".pth\"\n","  )"],"metadata":{"id":"AEsj-G3Fdchb","executionInfo":{"status":"ok","timestamp":1706293275379,"user_tz":-60,"elapsed":1896,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Entraînement"],"metadata":{"id":"B9FX_8tIdc3c"}},{"cell_type":"markdown","source":["Entraînement et sauvegarde du modèle"],"metadata":{"id":"Wsqssn5nesS7"}},{"cell_type":"code","source":["# @markdown - Nombre de entraînements à effectuer :\n","explorations = 2 # @param {type:\"integer\"}\n","# @markdown - Nombre de parties à jouer pour un seul entraînement :\n","batch_size = 2 # @param {type:\"integer\"}\n","# @markdown - Nombre d'époques d'apprentissage des réseaux de neurones par entraînement :\n","epochs = 5 # @param {type:\"integer\"}\n","# @markdown - Cocher cette case si vous souhaitez que le modèle entraîné soit sauvegardé sur le drive.\n","save = False # @param {type:\"boolean\"}"],"metadata":{"id":"xCVnd05helRI","executionInfo":{"status":"ok","timestamp":1706293275379,"user_tz":-60,"elapsed":10,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["if save:\n","  report = model.fit(\n","      explorations=explorations,\n","      max_actions=100,\n","      epochs=epochs,\n","      batch_size=batch_size,\n","      saving_path=\"code/models\",\n","      verbose=4\n","  )\n","else:\n","  report = model.fit(\n","      explorations=explorations,\n","      max_actions=100,\n","      epochs=epochs,\n","      batch_size=batch_size,\n","      saving_path=\"code/models\",\n","      verbose=4\n","  )"],"metadata":{"id":"y1xldPvSdi28","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706293275742,"user_tz":-60,"elapsed":369,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}},"outputId":"7628af68-117c-411a-84a7-b5a50f583f1d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------------------------------------------\n","- Exploration  1 / 2 :\n","Simulation  1 / 2\n","Simulation  2 / 2\n","Epoch  1 / 5\n","Epoch  2 / 5\n","Epoch  3 / 5\n","Epoch  4 / 5\n","Epoch  5 / 5\n","-----------------------------------------------------\n","- Exploration  2 / 2 :\n","Simulation  1 / 2\n","Simulation  2 / 2\n","Epoch  1 / 5\n","Epoch  2 / 5\n","Epoch  3 / 5\n","Epoch  4 / 5\n","Epoch  5 / 5\n","-----------------------------------------------------\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-7-03ecd6cdf3fe>:315: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  batch_acts_classifier = torch.tensor(batch_acts_classifier, dtype=torch.float)\n"]}]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"id":"Rcs6HdvBdjW4"}},{"cell_type":"markdown","source":["Evaluation du modèle"],"metadata":{"id":"kMWpcTLVe5tI"}},{"cell_type":"code","source":["rews, acts, wins, rend = model.evaluate(max_actions=1000, win_limit=100, games_limit=1000)\n","print(\"Nombre d'actions effectuées pendant toutes les parties de test :\", acts)\n","print(\"Total de toutes les récompenses :\",rews)\n","print(\"Nombre de parties gagnées :\", wins)\n","print(\"Récompense moyenne par action :\", rews/acts)"],"metadata":{"id":"-MHvURTsdlVv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706293277041,"user_tz":-60,"elapsed":1303,"user":{"displayName":"Leo Laiolo","userId":"14005185719469568547"}},"outputId":"44de10c3-7f6d-44b0-a302-16b196080f97"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Nombre d'actions effectuées pendant toutes les parties de test : 107\n","Total de toutes les récompenses : 4967.349821940147\n","Nombre de parties gagnées : 100\n","Récompense moyenne par action : 46.42383011159016\n"]}]},{"cell_type":"markdown","source":["## Illustration"],"metadata":{"id":"rbcl8mhw9dr2"}},{"cell_type":"markdown","source":["Aucune partie ne peut être illustrée sur Google Colab. Vous pouvez trouver une vidéo d'une partie de notre H-PPO entraîné dans le dossier `videos` du drive partagé."],"metadata":{"id":"uoJj3wfe9f-j"}}]}